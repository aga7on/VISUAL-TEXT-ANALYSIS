# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–î–æ–±–∞–≤–ª–µ–Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–µ—Ç–µ–≤—ã–º –æ—à–∏–±–∫–∞–º –∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É
"""

import re
import asyncio
import aiohttp
import requests
import os
import time
import random
import json
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Union
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Å–µ—Ç–µ–≤—ã–º –æ—à–∏–±–∫–∞–º
NETWORK_CONFIG = {
    'timeout': 30,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
    'max_retries': 3,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
    'backoff_factor': 1,  # –§–∞–∫—Ç–æ—Ä —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ—Ç–∫–∞—Ç–∞
    'retry_statuses': [429, 500, 502, 503, 504],  # –°—Ç–∞—Ç—É—Å—ã –¥–ª—è –ø–æ–≤—Ç–æ—Ä–∞
    'delay_between_requests': 1,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
}

def create_robust_session():
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é requests —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Å–µ—Ç–µ–≤—ã–º –æ—à–∏–±–∫–∞–º
    """
    session = requests.Session()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ retry —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    retry_strategy = Retry(
        total=NETWORK_CONFIG['max_retries'],
        status_forcelist=NETWORK_CONFIG['retry_statuses'],
        backoff_factor=NETWORK_CONFIG['backoff_factor'],
        raise_on_status=False
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

def safe_request(url, headers=None, params=None, timeout=None, session=None):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
    """
    if session is None:
        session = create_robust_session()
    
    if timeout is None:
        timeout = NETWORK_CONFIG['timeout']
    
    if headers is None:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    for attempt in range(NETWORK_CONFIG['max_retries'] + 1):
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limiting
            if attempt > 0:
                delay = NETWORK_CONFIG['delay_between_requests'] * (2 ** attempt) + random.uniform(0, 1)
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}, –æ–∂–∏–¥–∞–Ω–∏–µ {delay:.1f} —Å–µ–∫...")
                time.sleep(delay)
            
            response = session.get(url, headers=headers, params=params, timeout=timeout)
            
            if response.status_code == 200:
                return response
            elif response.status_code == 429:  # Rate limit
                print(f"Rate limit (429), –æ–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                time.sleep(5 + random.uniform(0, 5))
                continue
            elif response.status_code in NETWORK_CONFIG['retry_statuses']:
                print(f"–û—à–∏–±–∫–∞ {response.status_code}, –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞...")
                continue
            else:
                print(f"–û—à–∏–±–∫–∞ HTTP {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"–¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
            if attempt < NETWORK_CONFIG['max_retries']:
                continue
        except requests.exceptions.ConnectionError:
            print(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
            if attempt < NETWORK_CONFIG['max_retries']:
                continue
        except Exception as e:
            print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            if attempt < NETWORK_CONFIG['max_retries']:
                continue
    
    return None

def search_images(query: str, max_results: int = 4, search_engine: Union[str, List[str]] = "duckduckgo", searxng_url: str = "http://localhost:8080") -> List[Dict]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤ –∏ SearXNG
    """
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if isinstance(search_engine, list):
        all_results = []
        results_per_engine = max(1, max_results // len(search_engine))
        
        for engine in search_engine:
            try:
                print(f"–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ {engine}...")
                results = search_images_single(query, results_per_engine, engine, searxng_url)
                all_results.extend(results)
                print(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ {engine}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ {engine}: {e}")
                continue
        
        return all_results[:max_results]
    
    # –û–¥–∏–Ω–æ—á–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫
    return search_images_single(query, max_results, search_engine, searxng_url)

def search_images_single(query: str, max_results: int = 4, search_engine: str = "duckduckgo", searxng_url: str = "http://localhost:8080") -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–¥–∏–Ω –ø–æ–∏—Å–∫–æ–≤–∏–∫
    """
    if search_engine == "duckduckgo":
        return search_images_duckduckgo(query, max_results)
    elif search_engine == "pixabay":
        return search_images_pixabay(query, max_results)
    elif search_engine == "pinterest":
        return search_images_pinterest(query, max_results)
    elif search_engine == "searxng":
        return search_images_searxng(query, max_results, searxng_url)
    elif search_engine == "tenor":
        return search_images_tenor(query, max_results)
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo –∫–∞–∫ —Å–∞–º—ã–π —Å—Ç–∞–±–∏–ª—å–Ω—ã–π
        return search_images_duckduckgo(query, max_results)

def search_images_duckduckgo(query: str, max_results: int = 4) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ DuckDuckGo (—Å–∞–º—ã–π —Å—Ç–∞–±–∏–ª—å–Ω—ã–π)
    """
    try:
        try:
            from ddgs import DDGS
        except ImportError:
            try:
                from duckduckgo_search import DDGS
                print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –ø–∞–∫–µ—Ç duckduckgo-search")
            except ImportError:
                print("–û—à–∏–±–∫–∞: –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–∞–∫–µ—Ç ddgs –∏–ª–∏ duckduckgo-search")
                return []
        
        results = []
        time.sleep(NETWORK_CONFIG['delay_between_requests'])
        
        with DDGS() as ddgs:
            try:
                ddgs_images_gen = ddgs.images(
                    query,
                    max_results=max_results,
                    safesearch="moderate"
                )
                
                for r in ddgs_images_gen:
                    results.append({
                        'url': r.get('image', ''),
                        'title': r.get('title', ''),
                        'source': r.get('source', ''),
                        'thumbnail': r.get('thumbnail', ''),
                        'width': r.get('width', 0),
                        'height': r.get('height', 0),
                        'author': 'DuckDuckGo'
                    })
                    
            except Exception as e:
                if "202" in str(e) and "Ratelimit" in str(e):
                    print("DuckDuckGo rate limit, –æ–∂–∏–¥–∞–Ω–∏–µ...")
                    time.sleep(10 + random.uniform(0, 5))
                    try:
                        ddgs_images_gen = ddgs.images(
                            query,
                            max_results=max_results,
                            safesearch="moderate"
                        )
                        
                        for r in ddgs_images_gen:
                            results.append({
                                'url': r.get('image', ''),
                                'title': r.get('title', ''),
                                'source': r.get('source', ''),
                                'thumbnail': r.get('thumbnail', ''),
                                'width': r.get('width', 0),
                                'height': r.get('height', 0),
                                'author': 'DuckDuckGo'
                            })
                    except Exception as e2:
                        print(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ DuckDuckGo: {e2}")
                        raise e2
                else:
                    raise e
                
        return results
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}")
        return []

def search_images_pixabay(query: str, max_results: int = 4) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Pixabay
    """
    try:
        session = create_robust_session()
        
        search_url = f"https://pixabay.com/api/"
        params = {
            'key': '9656065-a4094594c34f9ac14c7fc4c39',
            'q': query,
            'image_type': 'photo',
            'per_page': max_results,
            'safesearch': 'true'
        }
        
        response = safe_request(search_url, params=params, session=session)
        
        if response and response.status_code == 200:
            data = response.json()
            results = []
            
            for hit in data.get('hits', [])[:max_results]:
                results.append({
                    'url': hit.get('webformatURL', ''),
                    'title': hit.get('tags', ''),
                    'source': 'Pixabay',
                    'thumbnail': hit.get('previewURL', ''),
                    'width': hit.get('webformatWidth', 0),
                    'height': hit.get('webformatHeight', 0),
                    'author': hit.get('user', 'Pixabay')
                })
            
            return results
        
        return []
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Pixabay: {e}")
        return []

def search_images_pinterest(query: str, max_results: int = 4) -> List[Dict]:
    """
    Pinterest –ø–æ–∏—Å–∫ —Å fallback
    """
    try:
        from playwright.sync_api import sync_playwright
        import urllib.parse
        
        encoded_query = urllib.parse.quote(query)
        search_url = f"https://www.pinterest.com/search/pins/?q={encoded_query}&rs=typed"
        
        print(f"–ü–æ–∏—Å–∫ Pinterest: {search_url}")
        results = []
        
        import asyncio
        if hasattr(asyncio, 'WindowsProactorEventLoopPolicy'):
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        
        with sync_playwright() as p:
            browser = p.firefox.launch(headless=True)
            try:
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0'
                )
                page = context.new_page()
                page.set_default_timeout(30000)
                page.goto(search_url, wait_until='networkidle')
                page.wait_for_timeout(3000)
                
                for i in range(7):
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    page.wait_for_timeout(1000)
                
                pin_containers = page.query_selector_all("div[data-test-id='pin']")
                processed_urls = set()
                
                for pin_container in pin_containers[:max_results * 2]:
                    try:
                        img_element = pin_container.query_selector("div[data-test-id='pinrep-image'] img.hCL")
                        if not img_element:
                            img_element = pin_container.query_selector("img[src*='/236x/'], img[src*='/474x/']")
                        
                        if img_element:
                            src = img_element.get_attribute('src')
                            if not src:
                                continue
                            
                            full_size_url = src
                            if '/236x/' in src:
                                full_size_url = src.replace('/236x/', '/originals/')
                            elif '/474x/' in src:
                                full_size_url = src.replace('/474x/', '/originals/')
                            
                            if full_size_url in processed_urls:
                                continue
                            processed_urls.add(full_size_url)
                            
                            img_alt = img_element.get_attribute('alt') or f"Pinterest image for {query}"
                            
                            results.append({
                                'url': full_size_url,
                                'title': img_alt,
                                'source': 'Pinterest',
                                'thumbnail': src,
                                'width': 0,
                                'height': 0,
                                'author': 'Pinterest'
                            })
                            
                            if len(results) >= max_results:
                                break
                                
                    except Exception as e:
                        continue
                
            finally:
                browser.close()
        
        return results
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Pinterest: {e}")
        return []

def search_images_searxng(query: str, max_results: int = 4, searxng_url: str = "http://localhost:8080") -> List[Dict]:
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô SearXNG –ø–æ–∏—Å–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HTML –ø–∞—Ä—Å–∏–Ω–≥ (–ü–†–û–¢–ï–°–¢–ò–†–û–í–ê–ù–û –ò –†–ê–ë–û–¢–ê–ï–¢!)
    –ö–ª—é—á–µ–≤–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º format=json, –ø–∞—Ä—Å–∏–º HTML
    """
    print(f"üîç SearXNG –ø–æ–∏—Å–∫: '{query}' —á–µ—Ä–µ–∑ {searxng_url}")
    
    # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å—ã + –ª–æ–∫–∞–ª—å–Ω—ã–π
    urls_to_try = []
    
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π URL, –ø—Ä–æ–±—É–µ–º –µ–≥–æ –ø–µ—Ä–≤—ã–º
    if searxng_url:
        urls_to_try.append(searxng_url)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É–±–ª–∏—á–Ω—ã–µ –∏–Ω—Å—Ç–∞–Ω—Å—ã –∫–∞–∫ fallback (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ localhost)
    if searxng_url != "http://localhost:8080":
        public_instances = [
            "https://searx.be",
            "https://searx.dresden.network", 
            "https://search.sapti.me",
            "https://searx.tiekoetter.com"
        ]
        urls_to_try.extend(public_instances)
    
    for attempt, base_url in enumerate(urls_to_try):
        try:
            print(f"üåê –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {base_url}")
            
            if attempt > 0:
                time.sleep(1 + random.uniform(0, 2))
            
            # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ —á–µ–º —Å–ª–æ–∂–Ω—ã–µ)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º format=json (–≤—ã–∑—ã–≤–∞–µ—Ç 403 Forbidden)
            search_params = {
                'q': query,
                'categories': 'images'
                # format=json –£–ë–†–ê–ù - —ç—Ç–æ –±—ã–ª–∞ –ø—Ä–∏—á–∏–Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã!
            }
            
            search_url = f"{base_url.rstrip('/')}/search"
            print(f"üì° HTML –∑–∞–ø—Ä–æ—Å: {search_url}")
            
            response = requests.get(search_url, params=search_params, headers=headers, timeout=20)
            print(f"üìä –û—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                # –ü–∞—Ä—Å–∏–º HTML –≤–º–µ—Å—Ç–æ JSON
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []
                processed_urls = set()
                
                print(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ò—â–µ–º –≤—Å–µ img —Ç–µ–≥–∏
                img_tags = soup.find_all('img')
                print(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ img —Ç–µ–≥–æ–≤: {len(img_tags)}")
                
                for img in img_tags:
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ URL
                        urls_to_check = []
                        if img.get('src') and img.get('src').startswith('http'):
                            urls_to_check.append(img.get('src'))
                        if img.get('data-src') and img.get('data-src').startswith('http'):
                            urls_to_check.append(img.get('data-src'))
                        
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π URL
                        img_src = None
                        for url in urls_to_check:
                            url_base = url.split('?')[0]
                            if url not in processed_urls and not any(url_base in r.get('url', '') for r in results):
                                img_src = url
                                break
                        
                        if img_src and img_src.startswith('http') and img_src not in processed_urls:
                            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            skip_patterns = ['/static/', 'favicon', 'logo', '/css/', '/js/', 'searx']
                            if any(pattern in img_src for pattern in skip_patterns):
                                continue
                            
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                            skip_domains = ['facebook.com', 'instagram.com', 'twitter.com', 'tiktok.com']
                            if any(domain in img_src for domain in skip_domains):
                                continue
                            
                            processed_urls.add(img_src)
                            
                            title = img.get('alt', '') or f"SearXNG result for {query}"
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                            engine = 'unknown'
                            parent = img.find_parent()
                            if parent:
                                engine_info = str(parent.get('class', []))
                                if 'bing' in engine_info.lower():
                                    engine = 'bing'
                                elif 'google' in engine_info.lower():
                                    engine = 'google'
                                elif 'yandex' in engine_info.lower():
                                    engine = 'yandex'
                            
                            result = {
                                'url': img_src,
                                'title': title[:100],
                                'source': f"SearXNG ({engine})",
                                'thumbnail': img_src,
                                'width': 0,
                                'height': 0,
                                'author': f"SearXNG via {engine}",
                                'engine': engine
                            }
                            
                            results.append(result)
                            print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç {len(results)}: {engine} - {title[:30]}...")
                            
                            if len(results) >= max_results:
                                break
                    
                    except Exception:
                        continue
                
                if results:
                    print(f"üéâ SearXNG —É—Å–ø–µ—à–Ω–æ: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {base_url}")
                    return results
                else:
                    print(f"‚ö†Ô∏è HTML –ø–æ–ª—É—á–µ–Ω, –Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    
            elif response.status_code == 403:
                print(f"‚ö†Ô∏è –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω –Ω–∞ {base_url}")
                continue
            else:
                print(f"‚ùå HTTP {response.status_code} –Ω–∞ {base_url}")
                continue
                
        except requests.exceptions.Timeout:
            print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ {base_url}")
            continue
        except requests.exceptions.ConnectionError:
            print(f"üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {base_url}")
            continue
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å {base_url}: {e}")
            continue
    
    # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ DuckDuckGo - —ç—Ç–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞–ª–æ –ø—Ä–æ–±–ª–µ–º—É
    print(f"‚ùå SearXNG: –≤—Å–µ –∏–Ω—Å—Ç–∞–Ω—Å—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
    print(f"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω—ã–π SearXNG –∑–∞–ø—É—â–µ–Ω –Ω–∞ {searxng_url}")
    
    return []


def search_images_external_searxng(query: str, max_results: int = 4) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–µ SearXNG –∏–Ω—Å—Ç–∞–Ω—Å—ã —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
    """
    print(f"üåê –í–Ω–µ—à–Ω–∏–π SearXNG –ø–æ–∏—Å–∫: '{query}'")
    
    # –°–ø–∏—Å–æ–∫ —Ä–∞–±–æ—á–∏—Ö –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
    external_instances = [
        "https://searx.dresden.network",
        "https://search.sapti.me", 
        "https://searx.tiekoetter.com",
        "https://searx.fmac.xyz"
    ]
    
    results = []
    
    for attempt, instance in enumerate(external_instances):
        try:
            print(f"üåê –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {instance}")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if attempt > 0:
                time.sleep(2 + random.uniform(0, 3))
            
            search_url = f"{instance.rstrip('/')}/search"
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            # –ü—Ä–æ–±—É–µ–º HTML –ø–∞—Ä—Å–∏–Ω–≥ (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ —á–µ–º JSON)
            html_params = {
                'q': query,
                'categories': 'images',
                'language': 'en'
            }
            
            response = requests.get(search_url, params=html_params, headers=headers, timeout=20)
            print(f"üìä –û—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                img_tags = soup.find_all('img')
                processed_urls = set()
                
                print(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ img —Ç–µ–≥–æ–≤: {len(img_tags)}")
                
                for img in img_tags:
                    if len(results) >= max_results:
                        break
                        
                    src = img.get('src') or img.get('data-src')
                    if src and src.startswith('http') and src not in processed_urls:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        skip_patterns = ['/static/', 'favicon', 'logo', 'searx', '/css/', '/js/', '/assets/']
                        if any(pattern in src for pattern in skip_patterns):
                            continue
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                        skip_domains = ['facebook.com', 'instagram.com', 'twitter.com', 'tiktok.com']
                        if any(domain in src for domain in skip_domains):
                            continue
                        
                        processed_urls.add(src)
                        
                        title = img.get('alt') or img.get('title') or f'External SearXNG result for {query}'
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ URL
                        engine = 'unknown'
                        if 'bing.net' in src or 'bing.com' in src:
                            engine = 'bing'
                        elif 'googleusercontent.com' in src or 'gstatic.com' in src:
                            engine = 'google'
                        elif 'yandex' in src:
                            engine = 'yandex'
                        elif 'wikipedia' in src:
                            engine = 'wikipedia'
                        
                        result = {
                            'url': src,
                            'title': title[:100],
                            'source': f"External SearXNG ({engine})",
                            'thumbnail': src,
                            'width': 0,
                            'height': 0,
                            'author': f"External SearXNG via {engine}",
                            'engine': engine
                        }
                        
                        results.append(result)
                        print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç {len(results)}: {engine} - {title[:30]}...")
                
                if results:
                    print(f"üéâ –í–Ω–µ—à–Ω–∏–π SearXNG —É—Å–ø–µ—à–Ω–æ: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {instance}")
                    return results
                else:
                    print(f"‚ö†Ô∏è HTML –ø–æ–ª—É—á–µ–Ω, –Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    
            elif response.status_code == 403:
                print(f"‚ö†Ô∏è –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω –Ω–∞ {instance}")
                continue
            elif response.status_code == 429:
                print(f"‚ö†Ô∏è Rate limit –Ω–∞ {instance}")
                continue
            else:
                print(f"‚ùå HTTP {response.status_code} –Ω–∞ {instance}")
                continue
                
        except requests.exceptions.Timeout:
            print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ {instance}")
            continue
        except requests.exceptions.ConnectionError:
            print(f"üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {instance}")
            continue
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å {instance}: {e}")
            continue
    
    print(f"‚ùå –í—Å–µ –≤–Ω–µ—à–Ω–∏–µ SearXNG –∏–Ω—Å—Ç–∞–Ω—Å—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
    return []


def search_images_tenor(query: str, max_results: int = 4) -> List[Dict]:
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ GIF/WebP/MP4 –Ω–∞ Tenor.com
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∫–ª—é—á–∞—è API
    """
    results = []
    encoded_query = quote(query)
    
    # –ú–µ—Ç–æ–¥ 1: –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Tenor API v2
    try:
        api_url = "https://tenor.googleapis.com/v2/search"
        api_params = {
            'q': query,
            'key': 'AIzaSyAyimkuYQYF_FXVALexPuGQctUWRURdCYQ',  # –ü—É–±–ª–∏—á–Ω—ã–π –∫–ª—é—á
            'limit': max_results,
            'media_filter': 'gif,webp'
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(api_url, params=api_params, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if 'results' in data:
                for item in data['results'][:max_results]:
                    media_formats = item.get('media_formats', {})
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: gif > webp > mp4
                    for format_name in ['gif', 'webp', 'mp4']:
                        if format_name in media_formats:
                            format_data = media_formats[format_name]
                            if isinstance(format_data, dict) and 'url' in format_data:
                                results.append({
                                    'url': format_data['url'],
                                    'title': item.get('content_description', f'Tenor {format_name} for {query}'),
                                    'source': 'Tenor',
                                    'thumbnail': format_data['url'],
                                    'width': format_data.get('dims', [0, 0])[0],
                                    'height': format_data.get('dims', [0, 0])[1],
                                    'author': 'Tenor',
                                    'type': format_name
                                })
                                break
                
                if results:
                    print(f"‚úÖ Tenor API v2: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    return results[:max_results]
    
    except Exception as e:
        print(f"Tenor API v2 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    # –ú–µ—Ç–æ–¥ 2: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π API endpoint
    try:
        alt_api_url = f"https://tenor.com/api/v1/search"
        alt_params = {
            'q': query,
            'key': 'LIVDSRZULELA',
            'limit': max_results
        }
        
        response = requests.get(alt_api_url, params=alt_params, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if 'results' in data:
                for item in data['results'][:max_results]:
                    media_formats = item.get('media_formats', {})
                    
                    for format_name in ['gif', 'webp', 'mp4', 'tinygif']:
                        if format_name in media_formats:
                            format_data = media_formats[format_name]
                            if isinstance(format_data, dict) and 'url' in format_data:
                                results.append({
                                    'url': format_data['url'],
                                    'title': item.get('title', f'Tenor {format_name} for {query}'),
                                    'source': 'Tenor',
                                    'thumbnail': format_data['url'],
                                    'width': format_data.get('dims', [0, 0])[0],
                                    'height': format_data.get('dims', [0, 0])[1],
                                    'author': 'Tenor',
                                    'type': format_name
                                })
                                break
                
                if results:
                    print(f"‚úÖ Tenor API v1: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    return results[:max_results]
    
    except Exception as e:
        print(f"Tenor API v1 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    # –ú–µ—Ç–æ–¥ 3: HTML –ø–∞—Ä—Å–∏–Ω–≥ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
    try:
        search_urls = [
            f"https://tenor.com/search/{encoded_query}-gifs",
            f"https://tenor.com/ru/search/{encoded_query}-gifs"
        ]
        
        session = create_robust_session()
        
        for search_url in search_urls:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ru,en;q=0.9',
                    'Referer': 'https://tenor.com/',
                    'DNT': '1',
                    'Connection': 'keep-alive'
                }
                
                response = safe_request(search_url, headers=headers, session=session)
                
                if response and response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    processed_urls = set()
                    
                    # –ü–æ–∏—Å–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
                    selectors = [
                        'img[src*="tenor.com"]',
                        'img[src*="media.tenor.com"]',
                        'img[data-src*="tenor.com"]',
                        'video[src*="tenor.com"]'
                    ]
                    
                    for selector in selectors:
                        elements = soup.select(selector)
                        
                        for element in elements:
                            src = element.get('src') or element.get('data-src')
                            
                            if src and src not in processed_urls and 'tenor' in src:
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                if any(skip in src for skip in ['/assets/', 'logo', 'icon']):
                                    continue
                                    
                                processed_urls.add(src)
                                
                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ–¥–∏–∞
                                media_type = 'gif'
                                if '.webp' in src:
                                    media_type = 'webp'
                                elif '.mp4' in src:
                                    media_type = 'mp4'
                                elif element.name == 'video':
                                    media_type = 'video'
                                
                                results.append({
                                    'url': src,
                                    'title': element.get('alt') or element.get('title') or f'Tenor {media_type} for {query}',
                                    'source': 'Tenor',
                                    'thumbnail': src,
                                    'width': 0,
                                    'height': 0,
                                    'author': 'Tenor',
                                    'type': media_type
                                })
                                
                                if len(results) >= max_results:
                                    break
                        
                        if len(results) >= max_results:
                            break
                    
                    # –ü–æ–∏—Å–∫ –≤ JavaScript –∫–æ–¥–µ
                    if len(results) < max_results:
                        scripts = soup.find_all('script')
                        for script in scripts:
                            script_text = script.get_text()
                            if script_text and ('tenor.com' in script_text or 'media.tenor.com' in script_text):
                                # –ò—â–µ–º JSON –¥–∞–Ω–Ω—ã–µ —Å URL
                                json_patterns = [
                                    r'"url":"(https://[^"]*tenor\.com[^"]*\.(?:gif|webp|mp4)[^"]*)"',
                                    r'"gif":\s*\{[^}]*"url":"([^"]*)"',
                                    r'"webp":\s*\{[^}]*"url":"([^"]*)"'
                                ]
                                
                                for pattern in json_patterns:
                                    matches = re.findall(pattern, script_text)
                                    for match in matches:
                                        if match not in processed_urls and 'tenor' in match:
                                            processed_urls.add(match)
                                            
                                            media_type = 'gif'
                                            if '.webp' in match:
                                                media_type = 'webp'
                                            elif '.mp4' in match:
                                                media_type = 'mp4'
                                            
                                            results.append({
                                                'url': match,
                                                'title': f'Tenor {media_type} for {query}',
                                                'source': 'Tenor',
                                                'thumbnail': match,
                                                'width': 0,
                                                'height': 0,
                                                'author': 'Tenor',
                                                'type': media_type
                                            })
                                            
                                            if len(results) >= max_results:
                                                break
                                    
                                    if len(results) >= max_results:
                                        break
                                
                                if len(results) >= max_results:
                                    break
                    
                    if results:
                        print(f"‚úÖ Tenor HTML: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                        return results[:max_results]
                        
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {search_url}: {e}")
                continue
    
    except Exception as e:
        print(f"HTML –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
    
    print(f"‚ùå Tenor: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")
    return []

# –§—É–Ω–∫—Ü–∏–∏ –∞–≤—Ç–æ–ø–æ–∏—Å–∫–∞ SearXNG –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ —É–¥–∞–ª–µ–Ω—ã –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Ç–µ–ø–µ—Ä—å –≤—ã–±–∏—Ä–∞—é—Ç –∏–Ω—Å—Ç–∞–Ω—Å—ã –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ https://searx.space/

# –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def download_images_async(image_urls, output_dir="images"):
    return []

def extract_urls_from_text(text):
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    return re.findall(url_pattern, text)

def search_images_from_urls(text, max_images_per_url=3):
    return []